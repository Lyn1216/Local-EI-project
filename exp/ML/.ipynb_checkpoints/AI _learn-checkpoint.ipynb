{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from func.iit_tpm import make_tpm, tpm_series, dis_mean\n",
    "from func.grn_tpm import text_bn_graph, iit_tpm_cal\n",
    "\n",
    "bnet = {\n",
    "    ('A', 'B'): 'w_ab',\n",
    "    ('B', 'A'): 'w_ba',\n",
    "    ('A', 'C'): 'w_ac',\n",
    "    ('C', 'A'): 'w_ca',\n",
    "    ('B', 'C'): 'w_bc',\n",
    "    ('C', 'B'): 'w_cb',\n",
    "    ('A', 'A'): 'w_a',\n",
    "    ('B', 'B'): 'w_b',\n",
    "    ('C', 'C'): 'w_c',\n",
    "    ('E1', 'A'): 'w1a',\n",
    "    ('E1', 'B'): 'w1b',\n",
    "    ('E1', 'C'): 'w1c',\n",
    "    ('E2', 'A'): 'w2a',\n",
    "    ('E2', 'B'): 'w2b',\n",
    "    ('E2', 'C'): 'w2c',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [01:02,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "n = 50\n",
    "syn_arr = np.zeros([n, n])\n",
    "un_sys_arr = np.zeros([n, n])\n",
    "un_en_arr = np.zeros([n, n])\n",
    "w_axis = np.linspace(0, 1, n)\n",
    "k_axis = np.linspace(0, 16, n)\n",
    "for w_id,w in tqdm(enumerate(w_axis)):\n",
    "    w1 = 0.5*(1 - w)\n",
    "    w2 = 1.5*(1 - w)\n",
    "    ww = {\n",
    "        'w_ab': w,\n",
    "        'w_ba': 0,\n",
    "        'w_ac': 0,\n",
    "        'w_ca': w,\n",
    "        'w_bc': w,\n",
    "        'w_cb': 0,\n",
    "        'w_a': w,\n",
    "        'w_b': w,\n",
    "        'w_c': w,\n",
    "        'w1a': w1,\n",
    "        'w1b': w1,\n",
    "        'w1c': w1,\n",
    "        'w2a': w2,\n",
    "        'w2b': w2,\n",
    "        'w2c': w2\n",
    "    }\n",
    "    for k_id,k in enumerate(k_axis):\n",
    "        tpm, tpm_v = make_tpm(bnet, ww, k=k)\n",
    "        un_sys, un_en, syn, tpm_dic = iit_tpm_cal(tpm_v, mech_size=3, en_size=2, dis=False) \n",
    "        syn_arr[k_id, w_id] = syn\n",
    "        un_sys_arr[k_id, w_id] = un_sys\n",
    "        un_en_arr[k_id, w_id] = un_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.50000000e-001, 2.50000000e-001, 2.50000000e-001, ...,\n",
       "        6.62599138e-262, 6.62599138e-262, 6.62599138e-262],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        2.50000000e-001, 2.50000000e-001, 2.50000000e-001],\n",
       "       [2.50000000e-001, 2.50000000e-001, 2.50000000e-001, ...,\n",
       "        1.12700677e-157, 1.12700677e-157, 1.12700677e-157],\n",
       "       ...,\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        2.50000000e-001, 2.50000000e-001, 2.50000000e-001],\n",
       "       [2.50000000e-001, 2.50000000e-001, 2.50000000e-001, ...,\n",
       "        1.91691202e-053, 1.91691202e-053, 1.91691202e-053],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        2.50000000e-001, 2.50000000e-001, 2.50000000e-001]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 0.2\n",
    "w1 = 0.5*(1 - w)\n",
    "w2 = 1.5*(1 - w)\n",
    "ww = {\n",
    "    'w_ab': w,\n",
    "    'w_ba': 0,\n",
    "    'w_ac': 0,\n",
    "    'w_ca': w,\n",
    "    'w_bc': w,\n",
    "    'w_cb': 0,\n",
    "    'w_a': w,\n",
    "    'w_b': w,\n",
    "    'w_c': w,\n",
    "    'w1a': w1,\n",
    "    'w1b': w1,\n",
    "    'w1c': w1,\n",
    "    'w2a': w2,\n",
    "    'w2b': w2,\n",
    "    'w2c': w2\n",
    "}\n",
    "tpm, tpm_v = make_tpm(bnet, ww, k=100)\n",
    "tpm_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94, 2), (94,), 0.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "colors = [\"#BB4F4F\", '#2A69B3', '#74B38F', '#9B76B2', \"#B99523\", \"#27B5AF\", \"#1E90FF\", \"#87CEEB\"]\n",
    "\n",
    "def decimal_to_binary(decimal, min_length=1):\n",
    "    if min_length == 0:\n",
    "        return ''\n",
    "    if decimal == 0:\n",
    "        return \"0\" if min_length == 1 else \"0\".zfill(min_length)\n",
    "    binary = \"\"\n",
    "    while decimal > 0:\n",
    "        binary = str(decimal % 2) + binary\n",
    "        decimal = decimal // 2\n",
    "    # 使用 zfill 确保二进制字符串至少有 min_length 长度\n",
    "    return binary.zfill(min_length)\n",
    "\n",
    "def tpm_series2(tpm, init_state, steps):\n",
    "    init_num = int(init_state, 2)\n",
    "    serie = [init_num]\n",
    "    serie_str = [init_state]\n",
    "    for t in range(steps):\n",
    "        num = serie[t]\n",
    "        probabilities = tpm[num, :]\n",
    "        sample = np.random.choice(range(len(probabilities)), p=probabilities)\n",
    "        serie.append(sample)\n",
    "        serie_str.append(decimal_to_binary(sample, min_length=int(np.log2(len(probabilities)))))\n",
    "    return serie, serie_str\n",
    "\n",
    "def serie_plot2(tpm, mech_size, en_size, en_series, init='111'):\n",
    "    un_sys, un_en, syn, tpm_dic = iit_tpm_cal(tpm, mech_size=mech_size, en_size=en_size, dis=False)   \n",
    "    strs = [decimal_to_binary(i, min_length=mech_size) for i in range(2**mech_size)] \n",
    "    long_serie = []\n",
    "    init_state = init\n",
    "    mark_serie = []\n",
    "    mark = 0\n",
    "    for en_list in en_series:\n",
    "        en_state = en_list[0]\n",
    "        steps = len(en_list)\n",
    "        mark += steps\n",
    "        mark_serie.append(mark)\n",
    "        serie, serie_str = tpm_series2(tpm_dic[en_state], init_state, steps)\n",
    "        init_state = serie_str[-1]\n",
    "        long_serie += serie[:-1]\n",
    "    return un_sys, un_en, syn, long_serie\n",
    "\n",
    "def generate_random_sequence(n, length, seeds):\n",
    "    res_ls = []\n",
    "    for seed in seeds:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        result = []\n",
    "        while len(result) < length:\n",
    "            num = random.randint(0, n)\n",
    "            if num not in result:\n",
    "                result.append(num)\n",
    "        res_ls.append(result)\n",
    "    return res_ls\n",
    "\n",
    "seeds = [41, 42]\n",
    "e_len = 2\n",
    "n = 12\n",
    "batch0 = (2**e_len * n - 1)\n",
    "samples = batch0 * len(seeds) \n",
    "train_input = np.zeros([samples, 2])\n",
    "train_target = np.zeros([samples])\n",
    "en_lss = generate_random_sequence(2**e_len-1, 2**e_len, seeds)\n",
    "for en_ind, en_ls0 in enumerate(en_lss):\n",
    "    en_series = [[decimal_to_binary(k, min_length=e_len) for _ in range(n)] for k in en_ls0]\n",
    "    n_sys, un_en, syn, long_serie = serie_plot2(tpm_v, 3, e_len, en_series, init='111')\n",
    "    en_num_arr = np.array([[k for _ in range(n)] for k in en_ls0]).reshape(-1)\n",
    "    long_arr = np.array(long_serie)\n",
    "    data_arr = np.vstack((long_arr, en_num_arr))\n",
    "    data_arr = data_arr[:, :-1].T\n",
    "    train_input[en_ind*batch0:(en_ind+1)*batch0, :] = data_arr\n",
    "    train_target[en_ind*batch0:(en_ind+1)*batch0] = long_arr[1:].T\n",
    "\n",
    "    \n",
    "train_input.shape, train_target.shape, syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.33277777497650773, 0.33277777497650773)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 0.4\n",
    "w1 = 0.5*(1 - w)\n",
    "w2 = 1.5*(1 - w)\n",
    "ww = {\n",
    "    'w_ab': w,\n",
    "    'w_ba': 0,\n",
    "    'w_ac': 0,\n",
    "    'w_ca': w,\n",
    "    'w_bc': w,\n",
    "    'w_cb': 0,\n",
    "    'w_a': w,\n",
    "    'w_b': w,\n",
    "    'w_c': w,\n",
    "    'w1a': w1,\n",
    "    'w1b': w1,\n",
    "    'w1c': w1,\n",
    "    'w2a': w2,\n",
    "    'w2b': w2,\n",
    "    'w2c': w2\n",
    "}\n",
    "tpm, tpm_v = make_tpm(bnet, ww, k=20)\n",
    "seeds = range(10)\n",
    "e_len = 2\n",
    "n = 60\n",
    "\n",
    "def make_data(tpm_v, n, e_len, seeds, init='111'):\n",
    "    batch0 = (2**e_len * n - 1)\n",
    "    samples = batch0 * len(seeds) \n",
    "    inputs = np.zeros([samples, 2])\n",
    "    target = np.zeros([samples])\n",
    "    en_lss = generate_random_sequence(2**e_len-1, 2**e_len, seeds)\n",
    "    for en_ind, en_ls0 in enumerate(en_lss):\n",
    "        en_series = [[decimal_to_binary(k, min_length=e_len) for _ in range(n)] for k in en_ls0]\n",
    "        n_sys, un_en, syn, long_serie = serie_plot2(tpm_v, 3, e_len, en_series, init=init)\n",
    "        en_num_arr = np.array([[k for _ in range(n)] for k in en_ls0]).reshape(-1)\n",
    "        long_arr = np.array(long_serie)\n",
    "        data_arr = np.vstack((long_arr, en_num_arr))\n",
    "        data_arr = data_arr[:, :-1].T\n",
    "        inputs[en_ind*batch0:(en_ind+1)*batch0, :] = data_arr\n",
    "        target[en_ind*batch0:(en_ind+1)*batch0] = long_arr[1:].T\n",
    "    return inputs, target, syn\n",
    "    \n",
    "train_input, train_target, syn0 = make_data(tpm_v, n, e_len, seeds, init='111')\n",
    "test_input, test_target, syn1 = make_data(tpm_v, n=10, e_len=2, seeds=[142], init='001')\n",
    "syn0, syn1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2000], Loss: 0.16595518589019775, Syn: 0.1475417967493159, test_loss: 0.039399050176143646\n",
      "Epoch [200/2000], Loss: 0.1295751929283142, Syn: 0.192363970794205, test_loss: 0.022013958543539047\n",
      "Epoch [300/2000], Loss: 0.1058264821767807, Syn: 0.2655892764784643, test_loss: 0.020521316677331924\n",
      "Epoch [400/2000], Loss: 0.09559085220098495, Syn: 0.320905438884027, test_loss: 0.018905440345406532\n",
      "Epoch [500/2000], Loss: 0.08908401429653168, Syn: 0.3551598161744962, test_loss: 0.018291691318154335\n",
      "Epoch [600/2000], Loss: 0.0843530222773552, Syn: 0.37701206860383496, test_loss: 0.018104806542396545\n",
      "Epoch [700/2000], Loss: 0.08050384372472763, Syn: 0.39931125005350676, test_loss: 0.01816832460463047\n",
      "Epoch [800/2000], Loss: 0.07700611650943756, Syn: 0.4265627202908485, test_loss: 0.018368739634752274\n",
      "Epoch [900/2000], Loss: 0.07384832203388214, Syn: 0.45640091094477553, test_loss: 0.017489885911345482\n",
      "Epoch [1000/2000], Loss: 0.07146342843770981, Syn: 0.4737117140275896, test_loss: 0.018139833584427834\n",
      "Epoch [1100/2000], Loss: 0.06982620805501938, Syn: 0.4860756513527211, test_loss: 0.0180167518556118\n",
      "Epoch [1200/2000], Loss: 0.06876137852668762, Syn: 0.4963335503298082, test_loss: 0.01815272681415081\n",
      "Epoch [1300/2000], Loss: 0.0680743083357811, Syn: 0.5049514621019432, test_loss: 0.01803300902247429\n",
      "Epoch [1400/2000], Loss: 0.06756803393363953, Syn: 0.5070716741940502, test_loss: 0.017439160495996475\n",
      "Epoch [1500/2000], Loss: 0.06724002957344055, Syn: 0.514167244493998, test_loss: 0.0181469414383173\n",
      "Epoch [1600/2000], Loss: 0.06700212508440018, Syn: 0.5177146580633707, test_loss: 0.017847709357738495\n",
      "Epoch [1700/2000], Loss: 0.06682800501585007, Syn: 0.5193048424122839, test_loss: 0.01798674277961254\n",
      "Epoch [1800/2000], Loss: 0.06670071184635162, Syn: 0.5216768483871876, test_loss: 0.01771562173962593\n",
      "Epoch [1900/2000], Loss: 0.06659623235464096, Syn: 0.5238792966528659, test_loss: 0.01764598675072193\n",
      "Epoch [2000/2000], Loss: 0.06651552766561508, Syn: 0.5259136764278424, test_loss: 0.01793406903743744\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64, 8)\n",
    "        self.layer3 = nn.Linear(64, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model = Classifier()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "func = torch.nn.LogSoftmax(dim=1)\n",
    "L = torch.nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "input_data = torch.tensor(train_input, dtype=float).float()\n",
    "targets = torch.tensor(train_target).long()\n",
    "test_input = torch.tensor(test_input, dtype=float).float() \n",
    "test_target = torch.tensor(test_target).long()\n",
    "# 训练循环\n",
    "\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播\n",
    "    outputs = model(input_data)\n",
    "    # 计算损失\n",
    "    y = func(outputs)\n",
    "    loss = L(y, targets)\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "    # 打印损失\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        syn_t = test_syn(model)\n",
    "        test_out = model(test_input)\n",
    "        y = func(test_out)\n",
    "        test_loss = L(y, test_target)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Syn: {syn_t}, test_loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_syn(model):\n",
    "    func = torch.nn.LogSoftmax(dim=0)\n",
    "    tpm_trained = np.ones([32, 8])\n",
    "    for i in range(8):\n",
    "        for j in range(4):\n",
    "            inputs = torch.tensor([i,j]).float()\n",
    "            tpm_trained[4*i+j,:] = np.exp(func(model(inputs)).data.numpy())\n",
    "\n",
    "    un_sys, un_en, syn, tpm_dic = iit_tpm_cal(tpm_trained, mech_size=3, en_size=2, dis=True) \n",
    "    return syn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
